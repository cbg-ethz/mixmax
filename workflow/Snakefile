include: Path("rules/common.smk")

rule all:
    input:
        output_files
    

checkpoint get_input_samples:
    output:
        samples_file = output_folder / "seed_{seed}" / "sample_list_{pool_size}.txt"
    params:
        number_of_samples = lambda wildcards: int(int(wildcards.pool_size)*(int(wildcards.pool_size) + 1)/2),
        loom_file_path = loom_file_path,
        seed = lambda w: w.seed,
    log:
        output_folder / 'log' / 'generate_input_samples_{seed}_{pool_size}.log'
    shell:
        """
        python {workflow.basedir}/scripts/get_input_samples.py \
        --number_of_samples {params.number_of_samples} \
        --loom_file_path {params.loom_file_path} \
        --output {output} \
        --seed {params.seed} \
        &> {log}
        """

rule split_files: ### This part of the pipeline is still not fully reproducible, because the files are split randomly. If I use the seed though, every file will be split the same way, which is not what I want.
    input:
        loom_file_path / '{loom_file}.loom',
    output:
        split1 = output_folder / "seed_{seed}" / "loom" / '{loom_file}_split1.loom',
        split2 = output_folder / "seed_{seed}" / "loom" / '{loom_file}_split2.loom',
    conda:
        "envs/loom.yml"
    resources:
        mem_mb_per_cpu=3000,
        runtime=90,
    log:
        output_folder / "seed_{seed}" / 'log' / 'split_{loom_file}.log',
    shell:
        """
        python {workflow.basedir}/scripts/split_loom_files.py \
         --input {input} \
        --alpha 6 \
        --output {output.split1} \
        &> {log}
        """


checkpoint create_demultiplexing_scheme:
    input:
        sample_file = output_folder / "seed_{seed}" / 'sample_list_{pool_size}.txt'
    output:
        pools = output_folder / "seed_{seed}" / "pools_{pool_size}_robust{robust}.txt"
    params:
        number_of_samples = lambda wildcards: int(int(wildcards.pool_size)*(int(wildcards.pool_size) + 1)/2),
        input_dir = lambda w: output_folder / f"seed_{w.seed}" / "loom"
    log:
        output_folder / "seed_{seed}" /  'log' / 'create_demultiplexing_scheme_{seed}_{pool_size}_robust{robust}.log'
    shell:
        """
        python {workflow.basedir}/scripts/create_demultiplexing_scheme.py \
        --robust {wildcards.robust} \
        --maximal_pool_size {wildcards.pool_size} \
        --n_samples {params.number_of_samples} \
        --output {output} \
        --input_sample_file {input.sample_file} \
        --input_dir {params.input_dir} \
        &> {log}
        """


rule pooling: ##Need to introduce proper downsampling of cells
    input:
        split_files = lambda w: yaml.safe_load(
            open(
                checkpoints.create_demultiplexing_scheme.get(seed=w.seed, pool_size = w.pool_size, robust = w.robust).output.pools
                )
            )[f"({w.pool_ID})"],
        pool_data = output_folder / "seed_{seed}" / "pools_{pool_size}_robust{robust}.txt",
    output:
        output_folder / "seed_{seed}" / paramspace.wildcard_pattern  / "pools" / "pool_{pool_ID}_{seed}.csv"
    params:
        minGQ=config.get("mosaic", {}).get("minGQ", 30),
        minDP=config.get("mosaic", {}).get("minDP", 10),
        minVAF=config.get("mosaic", {}).get("minVAF", 0.2),
        minVarGeno=config.get("mosaic", {}).get("minVarGeno", 0.5),
        minCellGeno=config.get("mosaic", {}).get("minCellGeno", 0.5),
        minMutated=config.get("mosaic", {}).get("minMutated", 0.05),  # default: 0.01
        maxRefVAF=config.get("mosaic", {}).get("maxRefVAF", 0.05),
        minHomVAF=config.get("mosaic", {}).get("minHomVAF", 0.95),
        minHetVAF=config.get("mosaic", {}).get("minHetVAF", 0.35),
        proximity=config.get("mosaic", {}).get("proximity", "25 50 100 200"),
        ratios = lambda w, input: sample_mixing_ratios(w.seed, len(input.split_files)),
    resources:
        mem_mb_per_cpu=10000,
        runtime=90,
    conda:
        "envs/mosaic.yml"
    log:
        output_folder / "seed_{seed}" / 'log' / 'pooling_{pool_ID}_{seed}_{doublet_rate}_{cell_count}_{pool_size}_robust{robust}.log'
    shell:
        """
        python {workflow.basedir}/scripts/mosaic_processing.py \
            --input {input.split_files} \
            -o {output} \
            --minGQ {params.minGQ} \
            --minDP {params.minDP} \
            --minVAF {params.minVAF} \
            --minVarGeno {params.minVarGeno} \
            --minCellGeno {params.minCellGeno} \
            --minMutated {params.minMutated} \
            --max_ref_VAF {params.maxRefVAF} \
            --min_hom_VAF {params.minHomVAF} \
            --min_het_VAF {params.minHetVAF} \
            --proximity {params.proximity} \
            --full_output \
            --ratio {params.ratios} \
            --doublets {wildcards.doublet_rate} \
            --cell_no {wildcards.cell_count} \
            &> {log}
        """


rule demultiplexing_demoTape:
        input:
            variants = output_folder /  "seed_{seed}" / paramspace.wildcard_pattern / "pools" / "pool_{pool_ID}_{seed}.csv",
            pools = output_folder / "seed_{seed}" / "pools_{pool_size}_robust{robust}.txt",
        output:
            profiles = output_folder / "seed_{seed}" / paramspace.wildcard_pattern / 'demultiplexed' / 'pool_{pool_ID}_{seed}_demultiplexed.profiles.tsv',
            assignments = output_folder / "seed_{seed}" / paramspace.wildcard_pattern / 'demultiplexed' / 'pool_{pool_ID}_{seed}_demultiplexed.assignments.tsv'
        params:
            cluster_number= lambda w, input: determine_number_of_different_donors(
                checkpoints.create_demultiplexing_scheme.get(seed=w.seed, pool_size = w.pool_size, robust = w.robust).output.pools,
                "({})".format(w.pool_ID)
                ),
            outbase = lambda w: output_folder / "seed_{}".format(w.seed) / paramspace.wildcard_pattern / 'demultiplexed' / "pool_{}_{}_demultiplexed".format(w.pool_ID, w.seed),
            output_folder = output_folder
        resources:
            mem_mb_per_cpu=4096,
            runtime=90,
        conda:
            "envs/sample_assignment.yml"
        log:
            output_folder / 'log' / 'demultiplexing_{pool_ID}_{seed}_{doublet_rate}_{cell_count}_{pool_size}_robust{robust}.log',
        shell:
            """
            python {workflow.basedir}/scripts/demultiplex_distance.py \
                --input {input.variants} \
                -n {params.cluster_number} \
                --output {params.output_folder}/seed_{wildcards.seed}/robust~{wildcards.robust}/pool_size~{wildcards.pool_size}/doublet_rate~{wildcards.doublet_rate}/cell_count~{wildcards.cell_count}/demultiplexed/pool_{wildcards.pool_ID}_{wildcards.seed}_demultiplexed  \
            &> {log}
           """


rule label_samples:
        input:
            demultiplexing_assignments = lambda wildcards: get_all_demultiplexed_assignments(wildcards, paramspace.wildcard_pattern),
        output:
            output_folder / "seed_{seed}" / paramspace.wildcard_pattern / 'sample_identity.yaml',
        resources:
            mem_mb_per_cpu=4096,
            runtime=90,
        conda:
            "envs/sample_assignment.yml"
        log:
            output_folder / 'log' / 'label_samples.{seed}_{doublet_rate}_{cell_count}_{pool_size}_robust{robust}.log',
        shell:
            """
            python {workflow.basedir}/scripts/assign_subsample_simulation.py \
                --demultiplexing_assignment {input.demultiplexing_assignments} \
                --output {output} \
            &> {log}
           """


rule assign_samples:
    input: 
        demultiplexing_genotypes = lambda wildcards: get_all_demultiplexed_profiles(wildcards, paramspace.wildcard_pattern),
        pools = output_folder / "seed_{seed}" / "pools_{pool_size}_robust{robust}.txt",
    output:
        sample_assignment = output_folder / "seed_{seed}" / paramspace.wildcard_pattern / 'sample_assignment.yaml',
        heatmap = output_folder / "seed_{seed}" / paramspace.wildcard_pattern / 'sample_assignment_heatmap.png',
    conda:
        "envs/sample_assignment.yml"
    log:
        output_folder / 'log' / 'assign_samples.{seed}_{doublet_rate}_{cell_count}_{pool_size}_robust{robust}.log',
    shell:
        """
        python {workflow.basedir}/scripts/compare_distances.py \
            --tsv_files {input.demultiplexing_genotypes} \
            --pool_scheme {input.pools} \
            --output_plot {output.heatmap} \
            --output {output.sample_assignment} \
            > {log}
        """