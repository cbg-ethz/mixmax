include: Path("rules/common.smk")

rule all:
    input:
        output_files
    

checkpoint get_input_samples:
    output:
        samples_file = output_folder / "seed_{seed}" / "sample_list_{seed}.txt"
    params:
        number_of_samples = number_of_samples,
        loom_file_path = loom_file_path,
        seed = lambda w: w.seed,
    log:
        output_folder / 'log' / 'generate_input_samples_{seed}.log'
    shell:
        """
        python {workflow.basedir}/scripts/get_input_samples.py \
        --number_of_samples {params.number_of_samples} \
        --loom_file_path {params.loom_file_path} \
        --output {output} \
        --seed {params.seed} \
        &> {log}
        """

rule split_files: ### This part of the pipeline is still not fully reproducible, because the files are split randomly. If I use the seed though, every file will be split the same way, which is not what I want.
    input:
        loom_file_path / '{loom_file}.loom',
    output:
        split1 = output_folder / "seed_{seed}" / "loom" / '{loom_file}_split1.loom',
        split2 = output_folder / "seed_{seed}" / "loom" / '{loom_file}_split2.loom',
    conda:
        "envs/loom.yml"
    resources:
        mem_mb_per_cpu=3000,
        runtime=90,
    log:
        output_folder / "seed_{seed}" / 'log' / 'split_{loom_file}.log',
    shell:
        """
        python {workflow.basedir}/scripts/split_loom_files.py \
         --input {input} \
        --alpha 6 \
        --output {output.split1} \
        &> {log}
        """


checkpoint create_demultiplexing_scheme:
    input:
        sample_file = output_folder / "seed_{seed}" / 'sample_list_{seed}.txt'
    output:
        pools = output_folder / "seed_{seed}" / "pools_{seed}.txt"
    params:
        maximal_pool_size = config["max_pool_size"],
        number_of_samples = number_of_samples,
        robust = False,
        input_dir = lambda w: output_folder / f"seed_{w.seed}" / "loom"
    log:
        output_folder / "seed_{seed}" /  'log' / 'create_demultiplexing_scheme_{seed}.log'
    shell:
        """
        python {workflow.basedir}/scripts/create_demultiplexing_scheme.py \
        --robust {params.robust} \
        --maximal_pool_size {params.maximal_pool_size} \
        --n_samples {params.number_of_samples} \
        --output {output} \
        --input_sample_file {input.sample_file} \
        --input_dir {params.input_dir} \
        &> {log}
        """


rule pooling: ##Need to introduce proper downsampling of cells
    input:
        split_files = lambda w: yaml.safe_load(
            open(
                checkpoints.create_demultiplexing_scheme.get(seed=w.seed).output.pools
                )
            )[f"({w.pool_ID})"],
        pool_data = output_folder / "seed_{seed}" / "pools_{seed}.txt",
    output:
        output_folder / "seed_{seed}" / "pools" / "pool_{pool_ID}_{seed}.csv"
    params:
        minGQ=config.get("mosaic", {}).get("minGQ", 30),
        minDP=config.get("mosaic", {}).get("minDP", 10),
        minVAF=config.get("mosaic", {}).get("minVAF", 0.2),
        minVarGeno=config.get("mosaic", {}).get("minVarGeno", 0.5),
        minCellGeno=config.get("mosaic", {}).get("minCellGeno", 0.5),
        minMutated=config.get("mosaic", {}).get("minMutated", 0.05),  # default: 0.01
        maxRefVAF=config.get("mosaic", {}).get("maxRefVAF", 0.05),
        minHomVAF=config.get("mosaic", {}).get("minHomVAF", 0.95),
        minHetVAF=config.get("mosaic", {}).get("minHetVAF", 0.35),
        proximity=config.get("mosaic", {}).get("proximity", "25 50 100 200"),
        ratios = lambda w, input: sample_mixing_ratios(w.seed, len(input.split_files)),
        doublet_rate = config["doublet_rate"],
        cell_number = config["downsampling"],
    resources:
        mem_mb_per_cpu=10000,
        runtime=90,
    conda:
        "envs/mosaic.yml"
    log:
        output_folder / "seed_{seed}" / 'log' / 'pooling_{pool_ID}_{seed}.log'
    shell:
        """
        python {workflow.basedir}/scripts/mosaic_processing.py \
            --input {input.split_files} \
            -o {output} \
            --minGQ {params.minGQ} \
            --minDP {params.minDP} \
            --minVAF {params.minVAF} \
            --minVarGeno {params.minVarGeno} \
            --minCellGeno {params.minCellGeno} \
            --minMutated {params.minMutated} \
            --max_ref_VAF {params.maxRefVAF} \
            --min_hom_VAF {params.minHomVAF} \
            --min_het_VAF {params.minHetVAF} \
            --proximity {params.proximity} \
            --full_output \
            --ratio {params.ratios} \
            --doublets {params.doublet_rate} \
            --cell_no {params.cell_number} \
            &> {log}
        """


rule demultiplexing_demoTape:
        input:
            variants = output_folder /  "seed_{seed}" / "pools" / "pool_{pool_ID}_{seed}.csv",
            pools = output_folder / "seed_{seed}" / "pools_{seed}.txt",
        output:
            profiles = output_folder / "seed_{seed}" / 'demultiplexed' / 'pool_{pool_ID}_{seed}_demultiplexed.profiles.tsv',
            assignments = output_folder / "seed_{seed}" / 'demultiplexed' / 'pool_{pool_ID}_{seed}_demultiplexed.assignments.tsv'
        resources:
            mem_mb_per_cpu=4096,
            runtime=90,
        conda:
            "envs/sample_assignment.yml"
        log:
            output_folder / 'log' / 'demultiplexing_{pool_ID}_{seed}.log',
        params:
            cluster_number= lambda w, input: determine_number_of_different_donors(
                checkpoints.create_demultiplexing_scheme.get(seed=w.seed).output.pools,
                "({})".format(w.pool_ID)
                ),
            outbase = lambda w: output_folder / "seed_{}".format(w.seed) / 'demultiplexed' / "pool_{}_{}_demultiplexed".format(w.pool_ID, w.seed)
        shell:
            """
            python {workflow.basedir}/scripts/demultiplex_distance.py \
                --input {input.variants} \
                -n {params.cluster_number} \
                --output {params.outbase} \
            &> {log}
           """


rule assign_samples:
        input:
            demultiplexing_assignments = output_folder / "seed_{seed}" / 'demultiplexed' / 'pool_{pool_ID}_{seed}_demultiplexed.assignments.tsv',
        output:
            output_folder / "seed_{seed}" / 'demultiplexed' / 'pool_{pool_ID}_{seed}_ground_truth_assignment.tsv',
        resources:
            mem_mb_per_cpu=4096,
            runtime=90,
        conda:
            "envs/sample_assignment.yml"
        log:
            output_folder / 'log' / 'assign_samples.{pool_ID}_{seed}.log',
        shell:
            """
            python {workflow.basedir}/scripts/assign_subsample_simulation.py \
                --demultiplexing_assignment {input.demultiplexing_assignments} \
                --output {output} \
            &> {log}
           """

#rule create_distance_matrix:
#    input: 
#        lambda w: get_demultiplexed_samples(output_folder / "seed_{}".format(seed=w.seed) / "pools_{}.txt".format(seed=w.seed)),